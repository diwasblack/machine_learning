\section{}
The network presented in \ref{sec:sgd} uses a sigmoid function as the activation function
of neurons. Using the same structure and initial weights given in \ref{sec:weights} we replace
the sigmoid function with a bipolar sigmoid function given in equation \ref{eq:bipolar_sigmoid}

\begin{equation}
\label{eq:bipolar_sigmoid}
 f(x)= \frac{2}{1 + e^{-x}} - 1
\end{equation}

\begin{equation}
\label{eq:bipolar_sigmoid_derivative}
 f'(x)= \frac{1}{2} (1 - f(x)^2)
\end{equation}

The input and output to the network are too converted into their bipolar representation. The
plot of training cost vs iterations is shown in figure \ref{fig:cost_bipolar}. Comparing 
figures \ref{fig:cost_sgd} and \ref{fig:cost_bipolar} we can see that the network using bipolar
sigmoid with bipolar XOR representation converged in fewer number of iterations then original
network.

\section{}
In order to evaluate the impact of Nguyen-Widrow approach of weight initialization on convergence
a neural network similar to the one described in \ref{sec:sgd} was constructed. Then the
sigmoid function was replaced with hyperbolic tangent function and bipolar representation of XOR
gate was used as the training data. The training cost with respect to the iterations is given in figure
\ref{fig:cost_tanh}. \newline

Above experiment was repeated after initializing the weights and biases as per the Nguyen-Widrow
approach. The plot of the training cost with respect iterations is shown in figure \ref{fig:cost_tanh_nw}.
Comparing figure \ref{fig:cost_tanh_nw} with figure \ref{fig:cost_tanh} it can seen that using Nguyen-Widrow approach
to initialize the weights smoothens the decrease in cost per iteration. Figure \ref{fig:tanh_nw_first_2500_compare} shows
the training cost for the first 2500 iteration of both cases.

\section{}
Initially a neural network with 3 units in first hidden layer and 2 units in second hidden layer was constructed.
The weights of the network is randomly initialized in the range $[-0.5, 0.5]$. As for the activation function,
hyperbolic tangent function was used and SGD with momentum coefficient($\mu$)$=0.5$ was used to optimize the parameters.
The plot for training cost with respect to iteration is given in figure \ref{fig:cost_two_hidden_layer}
